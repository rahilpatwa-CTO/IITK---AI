{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfeK4M8xapy2R1dP2HNpnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahilpatwa-CTO/IITK---AI/blob/main/Google_Mini_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rke"
      ],
      "metadata": {
        "id": "Y5cEgn0tTFZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVLyX5sC9YiC",
        "outputId": "881cc099-8264-4aac-9026-802c3e320077"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to your personal mini-Google! (type 'exit' to quit)\n",
            "\n",
            "\n",
            "Top results:\n",
            "\n",
            "Result #1\n",
            "Score: 0.2010\n",
            "From document: 3\n",
            "Chunk text: Social media posts and email newsletters support the SEO strategy. Performance is monitored using web analytics dashboards. The campaign will be iterated based on user engagement metrics.\n",
            "\n",
            "Result #2\n",
            "Score: 0.1730\n",
            "From document: 3\n",
            "Chunk text: The new marketing campaign focuses heavily on SEO and content marketing. We conducted keyword research to identify high-intent search terms. The team optimized landing pages for better click-through and conversion rates.\n",
            "\n",
            "Result #3\n",
            "Score: 0.0000\n",
            "From document: 0\n",
            "Chunk text: We recently launched a new AI analytics tool for business intelligence. The tool helps organizations analyze large volumes of operational data in real time. It provides interactive dashboards, anomaly detection, and predictive insights.\n",
            "\n",
            "Result #4\n",
            "Score: 0.0000\n",
            "From document: 0\n",
            "Chunk text: This AI analytics platform is designed for non-technical business users. Users can create custom reports using a simple drag-and-drop interface. The solution integrates with existing CRM and ERP systems.\n",
            "\n",
            "Result #5\n",
            "Score: 0.0000\n",
            "From document: 1\n",
            "Chunk text: The quarterly finance report shows a steady increase in revenue. Operating margins improved due to cost optimization initiatives. The finance team highlighted risks related to foreign exchange volatility.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# personal_search_engine.py\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- 1. Make sure NLTK has the tokenizer ---\n",
        "# Run these once (they will download tokenization models)\n",
        "nltk.download(\"punkt\")\n",
        "# For newer NLTK, you may also need:\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# --- 2. Define your small \"database\" of documents ---\n",
        "\n",
        "documents = [\n",
        "    # Doc 0: AI analytics tool\n",
        "    \"\"\"\n",
        "    We recently launched a new AI analytics tool for business intelligence.\n",
        "    The tool helps organizations analyze large volumes of operational data in real time.\n",
        "    It provides interactive dashboards, anomaly detection, and predictive insights.\n",
        "    This AI analytics platform is designed for non-technical business users.\n",
        "    Users can create custom reports using a simple drag-and-drop interface.\n",
        "    The solution integrates with existing CRM and ERP systems.\n",
        "    \"\"\",\n",
        "\n",
        "    # Doc 1: Finance report\n",
        "    \"\"\"\n",
        "    The quarterly finance report shows a steady increase in revenue.\n",
        "    Operating margins improved due to cost optimization initiatives.\n",
        "    The finance team highlighted risks related to foreign exchange volatility.\n",
        "    Shareholders were informed about a new dividend payout policy.\n",
        "    The CFO also discussed long-term capital allocation plans.\n",
        "    The report emphasizes the importance of disciplined financial planning.\n",
        "    \"\"\",\n",
        "\n",
        "    # Doc 2: Cloud infrastructure (AWS, Azure)\n",
        "    \"\"\"\n",
        "    Our cloud infrastructure runs on both AWS and Microsoft Azure.\n",
        "    We use AWS for compute-intensive workloads and Azure for analytics.\n",
        "    The architecture includes virtual networks, load balancers, and managed databases.\n",
        "    Security groups and network security policies are centrally managed.\n",
        "    We use infrastructure-as-code to automate provisioning and updates.\n",
        "    This hybrid-cloud design improves scalability, reliability, and cost control.\n",
        "    \"\"\",\n",
        "\n",
        "    # Doc 3: Marketing campaign (SEO)\n",
        "    \"\"\"\n",
        "    The new marketing campaign focuses heavily on SEO and content marketing.\n",
        "    We conducted keyword research to identify high-intent search terms.\n",
        "    The team optimized landing pages for better click-through and conversion rates.\n",
        "    Social media posts and email newsletters support the SEO strategy.\n",
        "    Performance is monitored using web analytics dashboards.\n",
        "    The campaign will be iterated based on user engagement metrics.\n",
        "    \"\"\",\n",
        "\n",
        "    # Doc 4: AI + machine learning together\n",
        "    \"\"\"\n",
        "    Our latest AI tool uses machine learning models to classify customer feedback.\n",
        "    The system identifies sentiment, topics, and intent in support tickets.\n",
        "    Machine learning helps prioritize issues that need urgent attention.\n",
        "    The AI tool continuously retrains on new labeled data from the support team.\n",
        "    This approach improves accuracy and adapts to changing customer language.\n",
        "    Over time, the machine learning pipeline becomes a core asset for the company.\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# --- 3. Chunk documents: sentence tokenize + group every 3 sentences ---\n",
        "\n",
        "def chunk_documents(docs, chunk_size=3):\n",
        "    chunks = []\n",
        "    for doc_id, doc in enumerate(docs):\n",
        "        sentences = sent_tokenize(doc.strip())\n",
        "        # Group every `chunk_size` sentences together\n",
        "        for i in range(0, len(sentences), chunk_size):\n",
        "            chunk_sentences = sentences[i:i + chunk_size]\n",
        "            chunk_text = \" \".join(chunk_sentences)\n",
        "            if chunk_text.strip():\n",
        "                chunks.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"chunk_id\": len(chunks),\n",
        "                    \"text\": chunk_text\n",
        "                })\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_documents(documents, chunk_size=3)\n",
        "\n",
        "# --- 4. Vectorize chunks using TF-IDF ---\n",
        "\n",
        "chunk_texts = [c[\"text\"] for c in chunks]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "chunk_vectors = vectorizer.fit_transform(chunk_texts)\n",
        "\n",
        "# --- 5. Function to search ---\n",
        "\n",
        "def search(query, top_k=5):\n",
        "    \"\"\"\n",
        "    Returns top_k chunks ranked by cosine similarity to the query.\n",
        "    \"\"\"\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, chunk_vectors).flatten()\n",
        "\n",
        "    scored_chunks = []\n",
        "    for score, chunk in zip(similarities, chunks):\n",
        "        scored_chunks.append({\n",
        "            \"score\": float(score),\n",
        "            \"doc_id\": chunk[\"doc_id\"],\n",
        "            \"chunk_id\": chunk[\"chunk_id\"],\n",
        "            \"text\": chunk[\"text\"]\n",
        "        })\n",
        "\n",
        "    # Sort by similarity score (descending)\n",
        "    scored_chunks.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    return scored_chunks[:top_k]\n",
        "\n",
        "# --- 6. Simple interactive loop ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to your personal mini-Google! (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your search query: \").strip()\n",
        "        if query.lower() in (\"exit\", \"quit\", \"\"):\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        results = search(query, top_k=5)\n",
        "\n",
        "        print(\"\\nTop results:\")\n",
        "        for rank, r in enumerate(results, start=1):\n",
        "            print(f\"\\nResult #{rank}\")\n",
        "            print(f\"Score: {r['score']:.4f}\")\n",
        "            print(f\"From document: {r['doc_id']}\")\n",
        "            print(f\"Chunk text: {r['text']}\")\n",
        "        print(\"\\n\" + \"-\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dapsaRI1Ezqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}